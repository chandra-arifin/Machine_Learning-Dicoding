# -*- coding: utf-8 -*-
"""Med_Submission1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yuDyUxYDHf-kkMIg2x_e5FPdWiqpS1xB

# Profile

Nama : Chandra Arifin <br/>
Project : Submission 1 <br/>
Machine Learning

# Import Libraries
"""

import tensorflow as tf
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from nltk.stem.snowball import SnowballStemmer

"""# Baca File dan Hapus Kolom"""

# baca dataset movies
df = pd.read_csv('movies.csv')

#penghapusan kolom yang tidak dipakai
df = df.drop(columns=['year', 'released', 'budget', 'gross', 'director', 'writer', 'star', 'country', 'company', 'runtime', 'votes',
                      'rating', 'score'])

#tampilkan record yang ada
df.head()

"""# Pemisahan Kategori dan Penyatuan Data"""

category = pd.get_dummies(df.genre)

newDF = pd.concat([df, category], axis=1)

newDF = newDF.drop(columns='genre')
newDF

stopper = set(stopwords.words('english'))

newDF['name'] = newDF['name'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stopper]))

def lemmatisasi(text):
    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

nltk.download('wordnet')
newDF['name'] = newDF['name'].apply(lemmatisasi)

namaFilm = newDF['name'].values
val = newDF[['Action', 'Adventure', 'Animation']].values

namaFilm

val

"""# Pelatihan dan Test"""

namaFilm_latih, namaFilm_test, val_latih, val_test = train_test_split(namaFilm, val, test_size=0.2)

namaFilm_latih

val_latih

"""# Tokenizer"""

token = Tokenizer(num_words=100, oov_token='x')
token.fit_on_texts(namaFilm_latih)
token.fit_on_texts(namaFilm_test)

seq_latih = token.texts_to_sequences(namaFilm_latih)
seq_test = token.texts_to_sequences(namaFilm_test)

pad_latih = pad_sequences(seq_latih)
pad_test = pad_sequences(seq_test)

"""# Pembuatan Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

"""# Pembuatan Model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=7668, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(32, activation='softmax'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

numEpoch = 100
# hist = model.fit(pad_latih, val_latih, epochs=numEpoch, validation_data=(pad_test, val_test), callbacks=[callbacks], verbose=2)
hist = model.fit(pad_latih, val_latih, epochs=numEpoch, validation_data=(pad_test, val_test), verbose=2)

"""# Pembuatan Grafik"""

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()